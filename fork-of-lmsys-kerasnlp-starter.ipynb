{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684,"modelId":2820}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":8265.260972,"end_time":"2024-06-03T19:18:50.009067","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-03T17:01:04.748095","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>","metadata":{"papermill":{"duration":0.014876,"end_time":"2024-06-03T17:01:07.523093","exception":false,"start_time":"2024-06-03T17:01:07.508217","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"我在这里写下我的笔记，这个比赛预测\"对特定的prompt进行用户偏向的response_a or response_b\"，这是一个多答案预测问题，给了55k的train_data，首通过csv先读取dataframe,然后对prompt和response_a和response_b三列进行eval()成真正的字符串和替换缺失值null为''，其次对这三列进行了encode()和decode()，所以有一个新的特征列\"encode_failed\",encode_fail.value_counts观察数据，其次对结果使用idxmax()打标签(0为win_model_a,1为win_model_b,2为win_model_tie)形成特征列\"class_name\"再通过CFG里的name2id字典，形成特征\"class_id\"，这样数据预处理大致完成\n\n接下来一个关键操作（Contextualize Response with Prompt）就是对每一个样本拼接 prompt+response_a ，拼接 prompt+response_b ，再concentrate形成一个新的特征option，具有两个seq\n\nEDA阶段拼接model探索LLM-Counts图像，也探索了Win-Counts图像，进行plt绘制\n\nbuild_dataset阶段 首先train_test_split()，把datafame82分为train_df和valid_df，\n一个关键操作就是DebertaV3Preprocessor()调用deberta_v3_extra_small_en模型把options处理成token_ids和padding_mask的字典(tokenize)，都是[2,512]的形式，token_ids是把seq根据语料库拆分成token再加上CLS和SEP映射为id，padding_mask用来标记是否是补位。\n然后加入shuffle把text和label(to_categorical)都传入dataset,\n\nLearnRate策略 均是不断降低 有利于不断接近最优解而不是在最优解附近反复横跳 目前已知策略：阶梯式降低(step)，指数级降低(设定lr_decay),余弦降低、\n先从一个lr_start在指定epoch内线性增加到lr_max，然后就选择策略降低到lr_min，这里使用了余弦降低，更加平滑一点\nModel Checkpointing设定为val_log_loss-》min，不断替换更好的模型，只保存权重文件，然后监测metrics也使用交叉熵（对数损失函数）\n\nModeling阶段也是我理解最久的一个阶段\n先定义输入：每一个输入都是一个字典（{token_ids:(2,512),padding_mask:(2,512)}）,\n然后通过切片[;,0,;]和[;,1,;](batch_size,seq,length)提取出response_a和response_b，然后通过调用deberta_v3_extra_small_en模型(shrae weights)(继承BERT架构)通过自注意力机制让[CLS]标记逐步融合全文本信息（生成嵌入向量）生成embed_a和embed_b，然后concentrate()两个嵌入向量进行一个GlobalAveragePooling1D成一维度（对比局部池化）,然后通过全连接层，三个神经元，非线性变换(softmax将神经元的输出转换为概率分布)添加一个全连接输出层预测构建模型\n\n然后就是编译模型  训练模型  预测","metadata":{}},{"cell_type":"markdown","source":"# Getting Started on LLM Classification Fine Tuning with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n\n<div align=\"center\">\n    <img src=\"https://i.ibb.co/wJMF5HL/lmsys.png\">\n</div>\n\nIn this competition, our aim is to predict which LLM responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). In other words, the goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. This notebook will guide you through the process of fine-tuning the **DebertaV3** model for this competition using the **Shared Weight** strategy with KerasNLP. This strategy is similar to how Multiple Choice Question (MCQ) models are trained. Additionally, we will use mixed precision for faster training and inference.\n\n**Did you know**: This notebook is backend-agnostic, which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of the preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n\n**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).\n","metadata":{"papermill":{"duration":0.014071,"end_time":"2024-06-03T17:01:07.551303","exception":false,"start_time":"2024-06-03T17:01:07.537232","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"pip install langdetect","metadata":{}},{"cell_type":"code","source":"pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:08.167668Z","iopub.execute_input":"2025-02-24T05:09:08.168276Z","iopub.status.idle":"2025-02-24T05:09:20.203312Z","shell.execute_reply.started":"2025-02-24T05:09:08.168235Z","shell.execute_reply":"2025-02-24T05:09:20.202268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📚 | Import Libraries ","metadata":{"papermill":{"duration":0.014736,"end_time":"2024-06-03T17:01:07.580011","exception":false,"start_time":"2024-06-03T17:01:07.565275","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras\nimport tensorflow as tf\n\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.express as px\n\nfrom langdetect import detect","metadata":{"_kg_hide-output":true,"papermill":{"duration":14.715162,"end_time":"2024-06-03T17:01:22.309231","exception":false,"start_time":"2024-06-03T17:01:07.594069","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:20.204885Z","iopub.execute_input":"2025-02-24T05:09:20.205261Z","iopub.status.idle":"2025-02-24T05:09:33.44266Z","shell.execute_reply.started":"2025-02-24T05:09:20.20522Z","shell.execute_reply":"2025-02-24T05:09:33.441931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Library Version","metadata":{"papermill":{"duration":0.014604,"end_time":"2024-06-03T17:01:22.338854","exception":false,"start_time":"2024-06-03T17:01:22.32425","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasNLP:\", keras_nlp.__version__)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023487,"end_time":"2024-06-03T17:01:22.377","exception":false,"start_time":"2024-06-03T17:01:22.353513","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:33.444407Z","iopub.execute_input":"2025-02-24T05:09:33.444949Z","iopub.status.idle":"2025-02-24T05:09:33.449735Z","shell.execute_reply.started":"2025-02-24T05:09:33.444919Z","shell.execute_reply":"2025-02-24T05:09:33.448663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚙️ | Configuration","metadata":{"papermill":{"duration":0.014156,"end_time":"2024-06-03T17:01:22.405533","exception":false,"start_time":"2024-06-03T17:01:22.391377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed = 42  # Random seed\n    preset = \"deberta_v3_extra_small_en\" # Name of pretrained models\n    sequence_length = 512  # Input sequence length\n    epochs = 3 # Training epochs\n    batch_size = 16  # Batch size\n    scheduler = 'cosine'  # Learning rate scheduler\n    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n    name2label = {v:k for k, v in label2name.items()}\n    class_labels = list(label2name.keys())\n    class_names = list(label2name.values())","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023008,"end_time":"2024-06-03T17:01:22.442868","exception":false,"start_time":"2024-06-03T17:01:22.41986","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:33.451209Z","iopub.execute_input":"2025-02-24T05:09:33.451656Z","iopub.status.idle":"2025-02-24T05:09:33.47819Z","shell.execute_reply.started":"2025-02-24T05:09:33.451607Z","shell.execute_reply":"2025-02-24T05:09:33.477353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ♻️ | Reproducibility \nSets value for random seed to produce similar result in each run.\n确保每次运行结果一样或者类似 随机种子\n神经网络权重初始化\n数据加载时的打乱（shuffle）\nDropout 层\n某些优化器的随机行为（如 Adam）\n通过固定种子，可以确保每次实验的结果一致，便于调试和对比不同模型的效果。","metadata":{"papermill":{"duration":0.014043,"end_time":"2024-06-03T17:01:22.471173","exception":false,"start_time":"2024-06-03T17:01:22.45713","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"papermill":{"duration":0.021526,"end_time":"2024-06-03T17:01:22.506961","exception":false,"start_time":"2024-06-03T17:01:22.485435","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:33.479158Z","iopub.execute_input":"2025-02-24T05:09:33.479399Z","iopub.status.idle":"2025-02-24T05:09:33.488988Z","shell.execute_reply.started":"2025-02-24T05:09:33.479376Z","shell.execute_reply":"2025-02-24T05:09:33.488087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🧮 | Mixed Precision\n\nIn this notebook, we will use mixed precision instead of float32 precision for training and inference to reduce GPU memory usage. This will ultimately allow us to use larger batch sizes, thus reducing our training and inference time.","metadata":{"papermill":{"duration":0.01407,"end_time":"2024-06-03T17:01:22.535249","exception":false,"start_time":"2024-06-03T17:01:22.521179","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"papermill":{"duration":0.020673,"end_time":"2024-06-03T17:01:22.570123","exception":false,"start_time":"2024-06-03T17:01:22.54945","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:33.490092Z","iopub.execute_input":"2025-02-24T05:09:33.490418Z","iopub.status.idle":"2025-02-24T05:09:33.499624Z","shell.execute_reply.started":"2025-02-24T05:09:33.490381Z","shell.execute_reply":"2025-02-24T05:09:33.499012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📁 | Dataset Path ","metadata":{"papermill":{"duration":0.013868,"end_time":"2024-06-03T17:01:22.598204","exception":false,"start_time":"2024-06-03T17:01:22.584336","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# BASE_PATH = '/kaggle/input/llm-classification-finetuning'\nBASE_PATH = '/kaggle/input/llm-classification-finetuning'\nprint(BASE_PATH)","metadata":{"papermill":{"duration":0.020401,"end_time":"2024-06-03T17:01:22.63286","exception":false,"start_time":"2024-06-03T17:01:22.612459","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:33.5009Z","iopub.execute_input":"2025-02-24T05:09:33.501495Z","iopub.status.idle":"2025-02-24T05:09:33.512191Z","shell.execute_reply.started":"2025-02-24T05:09:33.501457Z","shell.execute_reply":"2025-02-24T05:09:33.511318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📖 | Meta Data \n\nThe competition dataset comprises user interactions from the ChatBot Arena. In each interaction, a judge presents one or more prompts to two different large language models and then indicates which model provided the more satisfactory response. The training data contains `55,000` rows, with an expected `25,000` rows in the test set.\n\n## Files\n\n### `train.csv`\n- `id`: Unique identifier for each row.\n- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n\n### `test.csv`\n- `id`: Unique identifier for each row.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n\n> Note that each interaction may have multiple prompts and responses, but this notebook will use only **one prompt per interaction**. You can choose to use all prompts and responses. Additionally, prompts and responses in the dataframe are provided as string-formatted lists, so they need to be converted to literal lists using `eval()`.\n","metadata":{"papermill":{"duration":0.01414,"end_time":"2024-06-03T17:01:22.661285","exception":false,"start_time":"2024-06-03T17:01:22.647145","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"# Load Train Data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:33.515778Z","iopub.execute_input":"2025-02-24T05:09:33.516096Z","iopub.status.idle":"2025-02-24T05:09:36.683082Z","shell.execute_reply.started":"2025-02-24T05:09:33.516052Z","shell.execute_reply":"2025-02-24T05:09:36.68235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Data","metadata":{"papermill":{"duration":0.013838,"end_time":"2024-06-03T17:01:22.689785","exception":false,"start_time":"2024-06-03T17:01:22.675947","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# df['language'] = df['prompt'].apply(lambda x: detect(x) if x else 'unknown')\n# # 统计语言类型\n# language_counts = df['language'].value_counts()\n# print(language_counts)\n\n# Sample data\n# df = df.sample(frac=0.10)\n\nmissing_p = (df['prompt'] == 'null').sum()\nmissing_a = (df['response_a'] == 'null').sum()\nmissing_b = (df['response_a'] == 'null').sum()\n\n# 统计缺失值\nprint(missing_p)\nprint(missing_a)\nprint(missing_b)","metadata":{"_kg_hide-input":true,"papermill":{"duration":8.625467,"end_time":"2024-06-03T17:01:31.32943","exception":false,"start_time":"2024-06-03T17:01:22.703963","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:36.684061Z","iopub.execute_input":"2025-02-24T05:09:36.684313Z","iopub.status.idle":"2025-02-24T05:09:36.716531Z","shell.execute_reply.started":"2025-02-24T05:09:36.68429Z","shell.execute_reply":"2025-02-24T05:09:36.715664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Take the first prompt and its associated response  map()  lambda表达式\ndf[\"prompt\"] = df.prompt.map(lambda x: eval(x)[0])\ndf[\"response_a\"] = df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ndf[\"response_b\"] = df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# # Label conversion\ndf[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df.class_name.map(CFG.name2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:36.717506Z","iopub.execute_input":"2025-02-24T05:09:36.717757Z","iopub.status.idle":"2025-02-24T05:09:39.978995Z","shell.execute_reply.started":"2025-02-24T05:09:36.717734Z","shell.execute_reply":"2025-02-24T05:09:39.978297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show Sample\ndf.head(n=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:39.98015Z","iopub.execute_input":"2025-02-24T05:09:39.980784Z","iopub.status.idle":"2025-02-24T05:09:39.999102Z","shell.execute_reply.started":"2025-02-24T05:09:39.980744Z","shell.execute_reply":"2025-02-24T05:09:39.998139Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Data","metadata":{"papermill":{"duration":0.014609,"end_time":"2024-06-03T17:01:31.359794","exception":false,"start_time":"2024-06-03T17:01:31.345185","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load Test Data\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# Take the first prompt and response\ntest_df[\"prompt\"] = test_df.prompt.map(lambda x: eval(x)[0])\ntest_df[\"response_a\"] = test_df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ntest_df[\"response_b\"] = test_df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Show Sample\ntest_df.head(n=10)","metadata":{"papermill":{"duration":0.034633,"end_time":"2024-06-03T17:01:31.409034","exception":false,"start_time":"2024-06-03T17:01:31.374401","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:40.000016Z","iopub.execute_input":"2025-02-24T05:09:40.000263Z","iopub.status.idle":"2025-02-24T05:09:40.017238Z","shell.execute_reply.started":"2025-02-24T05:09:40.000239Z","shell.execute_reply":"2025-02-24T05:09:40.016306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Contextualize Response with Prompt\n\nIn our approach, we will contextualize each response with the prompt instead of using a single prompt for all responses. This means that for each response, we will provide the model with the same set of prompts combined with their respective response (e.g., `(P + R_A)`, `(P + R_B)`, etc.). This approach is similar to the multiple-choice question task in NLP.\n\n> Note that some prompts and responses may not be encoded with `utf-8`, resulting in errors when creating the dataloader. In such cases, we will replace them with an empty string.\n","metadata":{"papermill":{"duration":0.014711,"end_time":"2024-06-03T17:01:31.438974","exception":false,"start_time":"2024-06-03T17:01:31.424263","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define a function to create options based on the prompt and choices\ndef make_pairs(row):\n    row[\"encode_fail\"] = False\n    try:\n        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        prompt = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_a = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_b = \"\"\n        row[\"encode_fail\"] = True\n    # <class 'list<<class 'str'>>'>\n    row['options'] = [f\"Prompt: {prompt}\\n\\nResponse: {response_a}\",  # Response from Model A\n                      f\"Prompt: {prompt}\\n\\nResponse: {response_b}\"  # Response from Model B\n                     ]\n    return row\n","metadata":{"papermill":{"duration":0.023596,"end_time":"2024-06-03T17:01:31.477321","exception":false,"start_time":"2024-06-03T17:01:31.453725","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:40.018402Z","iopub.execute_input":"2025-02-24T05:09:40.019378Z","iopub.status.idle":"2025-02-24T05:09:40.025068Z","shell.execute_reply.started":"2025-02-24T05:09:40.019347Z","shell.execute_reply":"2025-02-24T05:09:40.024209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(df.head())  # Display the first 2 rows of df\n\ntest_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(test_df.head())  # Display the first 2 rows of df","metadata":{"papermill":{"duration":62.732686,"end_time":"2024-06-03T17:02:34.262915","exception":false,"start_time":"2024-06-03T17:01:31.530229","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:09:40.026092Z","iopub.execute_input":"2025-02-24T05:09:40.026394Z","iopub.status.idle":"2025-02-24T05:10:32.599858Z","shell.execute_reply.started":"2025-02-24T05:09:40.026369Z","shell.execute_reply":"2025-02-24T05:10:32.598929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoding Fail Statistics\n\nLet's examine how many samples have encoding issues. From the code below, we can see that only $1\\%$ of the samples failed to be encoded, while $99\\%$ of the samples don't have any issues. A similar pattern can be expected for the test data as well. Thus, considering empty strings for this small portion of the data will not have much impact on our training and inference.","metadata":{"papermill":{"duration":0.015064,"end_time":"2024-06-03T17:02:34.293411","exception":false,"start_time":"2024-06-03T17:02:34.278347","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df.encode_fail.value_counts(normalize=False)","metadata":{"papermill":{"duration":0.032427,"end_time":"2024-06-03T17:02:34.341044","exception":false,"start_time":"2024-06-03T17:02:34.308617","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:32.600859Z","iopub.execute_input":"2025-02-24T05:10:32.601121Z","iopub.status.idle":"2025-02-24T05:10:32.615025Z","shell.execute_reply.started":"2025-02-24T05:10:32.601095Z","shell.execute_reply":"2025-02-24T05:10:32.613964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🎨 | Exploratory Data Analysis (EDA)","metadata":{"papermill":{"duration":0.015466,"end_time":"2024-06-03T17:02:34.372077","exception":false,"start_time":"2024-06-03T17:02:34.356611","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## LLM Distribution","metadata":{"papermill":{"duration":0.015944,"end_time":"2024-06-03T17:02:34.404115","exception":false,"start_time":"2024-06-03T17:02:34.388171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_df = pd.concat([df.model_a, df.model_b])\ncounts = model_df.value_counts().reset_index()\ncounts.columns = ['LLM', 'Count']\n\n# Create a bar plot with custom styling using Plotly\nfig = px.bar(counts, x='LLM', y='Count',\n             title='Distribution of LLMs',\n             color='Count', color_continuous_scale='viridis')\n\nfig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability\n\nfig.show(renderer='iframe_connected')","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.475495,"end_time":"2024-06-03T17:02:35.895386","exception":false,"start_time":"2024-06-03T17:02:34.419891","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:32.616153Z","iopub.execute_input":"2025-02-24T05:10:32.616437Z","iopub.status.idle":"2025-02-24T05:10:33.999449Z","shell.execute_reply.started":"2025-02-24T05:10:32.616412Z","shell.execute_reply":"2025-02-24T05:10:33.998602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Winning Distribution","metadata":{"papermill":{"duration":0.016453,"end_time":"2024-06-03T17:02:35.928279","exception":false,"start_time":"2024-06-03T17:02:35.911826","status":"completed"},"tags":[]}},{"cell_type":"code","source":"counts = df['class_name'].value_counts().reset_index()\ncounts.columns = ['Winner', 'Win Count']\n\nfig = px.bar(counts, x='Winner', y='Win Count',\n             title='Winner distribution for Train Data',\n             labels={'Winner': 'Winner', 'Win Count': 'Win Count'},\n             color='Winner', color_continuous_scale='viridis')\n\nfig.update_layout(xaxis_title=\"Winner\", yaxis_title=\"Win Count\")\n\nfig.show(renderer='iframe_connected')\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.112531,"end_time":"2024-06-03T17:02:36.056818","exception":false,"start_time":"2024-06-03T17:02:35.944287","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:34.000681Z","iopub.execute_input":"2025-02-24T05:10:34.001099Z","iopub.status.idle":"2025-02-24T05:10:34.074139Z","shell.execute_reply.started":"2025-02-24T05:10:34.001062Z","shell.execute_reply":"2025-02-24T05:10:34.073304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔪 | Data Split\n\nIn the code snippet provided below, we will divide the existing data into training and validation using a stratification of `class_label` column.","metadata":{"papermill":{"duration":0.016208,"end_time":"2024-06-03T17:02:36.090042","exception":false,"start_time":"2024-06-03T17:02:36.073834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # Import package\n\ntrain_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"class_label\"])","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.779715,"end_time":"2024-06-03T17:02:36.885945","exception":false,"start_time":"2024-06-03T17:02:36.10623","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:34.075332Z","iopub.execute_input":"2025-02-24T05:10:34.076113Z","iopub.status.idle":"2025-02-24T05:10:34.532773Z","shell.execute_reply.started":"2025-02-24T05:10:34.076074Z","shell.execute_reply":"2025-02-24T05:10:34.532082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🍽️ | Preprocessing对文本数据进行预处理\n\n将文本数据转换为适合输入到 DeBERTa V3 模型的格式\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{"papermill":{"duration":0.016189,"end_time":"2024-06-03T17:02:36.919019","exception":false,"start_time":"2024-06-03T17:02:36.90283","status":"completed"},"tags":[]}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n    preset=CFG.preset, # Name of the model\n    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n)","metadata":{"papermill":{"duration":3.390834,"end_time":"2024-06-03T17:02:40.326479","exception":false,"start_time":"2024-06-03T17:02:36.935645","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:34.534126Z","iopub.execute_input":"2025-02-24T05:10:34.535049Z","iopub.status.idle":"2025-02-24T05:10:41.159855Z","shell.execute_reply.started":"2025-02-24T05:10:34.535009Z","shell.execute_reply":"2025-02-24T05:10:41.158855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\\_responses, sequence\\_length)$.","metadata":{"papermill":{"duration":0.016599,"end_time":"2024-06-03T17:02:40.360813","exception":false,"start_time":"2024-06-03T17:02:40.344214","status":"completed"},"tags":[]}},{"cell_type":"code","source":"outs = preprocessor(df.options.iloc[0])  # Process options for the first row\n\n# Display the shape of each processed output\nfor k, v in outs.items():\n    print(k, \":\", v)\n    print(k, \":\", v.shape)","metadata":{"papermill":{"duration":1.056025,"end_time":"2024-06-03T17:02:41.433464","exception":false,"start_time":"2024-06-03T17:02:40.377439","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:41.161016Z","iopub.execute_input":"2025-02-24T05:10:41.161366Z","iopub.status.idle":"2025-02-24T05:10:42.111092Z","shell.execute_reply.started":"2025-02-24T05:10:41.161331Z","shell.execute_reply":"2025-02-24T05:10:42.1101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.","metadata":{"papermill":{"duration":0.018502,"end_time":"2024-06-03T17:02:41.469384","exception":false,"start_time":"2024-06-03T17:02:41.450882","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_fn(text, label=None):\n    text = preprocessor(text)  # Preprocess text\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"papermill":{"duration":0.024249,"end_time":"2024-06-03T17:02:41.510214","exception":false,"start_time":"2024-06-03T17:02:41.485965","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:42.112388Z","iopub.execute_input":"2025-02-24T05:10:42.112725Z","iopub.status.idle":"2025-02-24T05:10:42.117406Z","shell.execute_reply.started":"2025-02-24T05:10:42.112694Z","shell.execute_reply":"2025-02-24T05:10:42.116564Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🍚 | DataLoader\n\nThe code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.\n\nTo learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).\n这段代码定义了一个函数 build_dataset，用于构建 TensorFlow 的数据管道（tf.data.Dataset）。它的作用是将输入的文本数据（texts）和标签数据（labels，如果有）转换为一个高效的数据集对象，\ntexts:\n输入的文本数据，通常是一个列表或数组，例如 [\"text1\", \"text2\", \"text3\"]。\nlabels:\n可选的标签数据，通常是一个列表或数组，例如 [0, 1, 2]。\n如果未提供标签（labels=None），则只处理文本数据。\nbatch_size:\n每个批次的大小，默认值为 32。\ncache:\n是否缓存数据集，默认值为 True。\n缓存可以加速数据加载，尤其是在数据预处理较慢时。\nshuffle:\n是否打乱数据集，默认值为 1024。\n如果设置为 False 或 None，则不进行打乱。\n如果设置为整数（如 1024），则使用该值作为缓冲区大小进行打乱。","metadata":{"papermill":{"duration":0.016686,"end_time":"2024-06-03T17:02:41.543394","exception":false,"start_time":"2024-06-03T17:02:41.526708","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_dataset(texts, labels=None, batch_size=32,\n                  cache=True, shuffle=1024):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n    # 将标签转换为 one-hot 编码格式（使用 keras.utils.to_categorical），并将文本和标签组合成一个元组 (texts, labels)。\n    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices\n    \n    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices 可迭代的数据结构\n\n     # data = ([\"text1\", \"text2\", \"text3\"], [0, 1, 2])\n     # ds = tf.data.Dataset.from_tensor_slices(data)\n     # ds 是一个数据集对象，包含 3 个样本，每个样本是一个元组：(\"text1\", 0), (\"text2\", 1), (\"text3\", 2)\n    \n    ds = ds.cache() if cache else ds  # Cache dataset if enabled 缓存数据集\n    \n    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function 并行处理数据，以优化性能\n    # ({\"tokens_id\":\"\"  ,  padding_masks\"\":\"\"},label),\n    \n    opt = tf.data.Options()  # Create dataset options\n    # 打乱数据集\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)  # Set dataset options\n    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset\n    ds = ds.prefetch(AUTO)  # Prefetch next batch 预取下一批数据，以进一步优化数据加载性能。\n    return ds  # Return the built dataset","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.026326,"end_time":"2024-06-03T17:02:41.586658","exception":false,"start_time":"2024-06-03T17:02:41.560332","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:42.118514Z","iopub.execute_input":"2025-02-24T05:10:42.11877Z","iopub.status.idle":"2025-02-24T05:10:42.127694Z","shell.execute_reply.started":"2025-02-24T05:10:42.118746Z","shell.execute_reply":"2025-02-24T05:10:42.127023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Train/Valid Dataloader","metadata":{"papermill":{"duration":0.016559,"end_time":"2024-06-03T17:02:41.619713","exception":false,"start_time":"2024-06-03T17:02:41.603154","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train\ntrain_texts = train_df.options.tolist()  # Extract training texts\ntrain_labels = train_df.class_label.tolist()  # Extract training labels\ntrain_ds = build_dataset(train_texts, train_labels,\n                         batch_size=CFG.batch_size,\n                         shuffle=True)\n\n# Valid\nvalid_texts = valid_df.options.tolist()  # Extract validation texts\nvalid_labels = valid_df.class_label.tolist()  # Extract validation labels\nvalid_ds = build_dataset(valid_texts, valid_labels,\n                         batch_size=CFG.batch_size,\n                         shuffle=False)","metadata":{"_kg_hide-input":false,"papermill":{"duration":5.308478,"end_time":"2024-06-03T17:02:46.944786","exception":false,"start_time":"2024-06-03T17:02:41.636308","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:42.131578Z","iopub.execute_input":"2025-02-24T05:10:42.131898Z","iopub.status.idle":"2025-02-24T05:10:46.97053Z","shell.execute_reply.started":"2025-02-24T05:10:42.131872Z","shell.execute_reply":"2025-02-24T05:10:46.969853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚓ | LR Schedule\n\nImplementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:\n- `step`: Lowering the learning rate in step-wise manner resembling stairs.\n- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.\n- `exp`: Exponentially decreasing the learning rate.\n\n**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.","metadata":{"papermill":{"duration":0.017157,"end_time":"2024-06-03T17:02:46.979278","exception":false,"start_time":"2024-06-03T17:02:46.962121","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start # 线性增加\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max # 保持不变\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min # 指数衰减\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2) # 阶梯指数衰减\n        elif mode == 'cos':  #余弦衰减\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.030355,"end_time":"2024-06-03T17:02:47.026587","exception":false,"start_time":"2024-06-03T17:02:46.996232","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:46.971673Z","iopub.execute_input":"2025-02-24T05:10:46.972035Z","iopub.status.idle":"2025-02-24T05:10:46.979844Z","shell.execute_reply.started":"2025-02-24T05:10:46.972Z","shell.execute_reply":"2025-02-24T05:10:46.978815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, plot=True)","metadata":{"papermill":{"duration":0.302348,"end_time":"2024-06-03T17:02:47.345856","exception":false,"start_time":"2024-06-03T17:02:47.043508","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:46.981211Z","iopub.execute_input":"2025-02-24T05:10:46.982032Z","iopub.status.idle":"2025-02-24T05:10:47.259966Z","shell.execute_reply.started":"2025-02-24T05:10:46.981993Z","shell.execute_reply":"2025-02-24T05:10:47.259136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 💾 | Model Checkpointing\n\nThe following code will create a callback that will save the best checkpoint of the model during training, which we will use for inference in the submission.","metadata":{"papermill":{"duration":0.017351,"end_time":"2024-06-03T17:02:47.381369","exception":false,"start_time":"2024-06-03T17:02:47.364018","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(f'model.weights.h5',\n                                          monitor='val_log_loss',\n                                          save_best_only=True,\n                                          save_weights_only=True,\n                                          mode='min')  # Get Model checkpoint callback","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.025064,"end_time":"2024-06-03T17:02:47.424145","exception":false,"start_time":"2024-06-03T17:02:47.399081","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:47.261386Z","iopub.execute_input":"2025-02-24T05:10:47.262088Z","iopub.status.idle":"2025-02-24T05:10:47.266937Z","shell.execute_reply.started":"2025-02-24T05:10:47.262049Z","shell.execute_reply":"2025-02-24T05:10:47.266085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📏 | Metric\n\nThe metric for this competition is **Log Loss**. This metric can be expressed mathematically as,\n\n$$\n\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)\n$$\n\nwhere $ N $ is the number of samples, $ y_i $ is the true label, and $ p_i $ is the predicted probability of the sample belonging to the positive class.\n\nNote that this metric is similar to categorical cross entropy widely used in classification tasks. Thus, we don't need to implement the loss from scratch. As the Keras library already has an implementation of this metric, we will simply use the metric to monitor performance of our model.\n","metadata":{"papermill":{"duration":0.017033,"end_time":"2024-06-03T17:02:47.458564","exception":false,"start_time":"2024-06-03T17:02:47.441531","status":"completed"},"tags":[]}},{"cell_type":"code","source":"log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")","metadata":{"papermill":{"duration":0.043053,"end_time":"2024-06-03T17:02:47.519019","exception":false,"start_time":"2024-06-03T17:02:47.475966","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:47.268086Z","iopub.execute_input":"2025-02-24T05:10:47.268418Z","iopub.status.idle":"2025-02-24T05:10:47.300556Z","shell.execute_reply.started":"2025-02-24T05:10:47.268391Z","shell.execute_reply":"2025-02-24T05:10:47.299841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🤖 | Modeling\n\nThe `KerasNLP` library provides various NLP model architectures such as `Bert`, `Roberta`, `DebertaV3`, and more. While this notebook focuses on `DebertaV3`, you can explore others in the [KerasNLP documentation](https://keras.io/api/keras_nlp/models/). For a deeper understanding, refer to the [getting started guide](https://keras.io/guides/keras_nlp/getting_started/).\n\nOur approach utilizes `keras_nlp.models.DebertaV3Classifier` to process each prompt and response pair, generating output embeddings. We then concatenate these embeddings and pass them through a Pooling layer and a classifier to obtain logits, followed by a `softmax` function for the final output.\n\nWhen dealing with multiple responses, we use a weight-sharing strategy. This means we provide the model with one response at a time along with the prompt `(P + R_A)`, `(P + R_B)`, etc., using the same model weights for all responses. After obtaining embeddings for all responses, we concatenate them and apply average pooling. Next, we use a `Linear/Dense` layer along with the `Softmax` function as the classifier for the final result. Providing all responses at once would increase text length and complicate model handling. Note that, in the classifier, we use 3 classes for `winner_model_a`, `winner_model_b`, and `draw` cases.\n\nThe diagram below illustrates this approach:\n\n<div align=\"center\">\n    <img src=\"https://i.postimg.cc/g0gcvy3f/Kaggle-drawio.png\">\n</div>\n\nFrom a coding perspective, note that we use the same model for all responses with shared weights, contrary to the separate models implied in the diagram.","metadata":{"papermill":{"duration":0.017265,"end_time":"2024-06-03T17:02:47.553968","exception":false,"start_time":"2024-06-03T17:02:47.536703","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define input layers\ninputs = {\n    \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n    \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"),\n}\n\n# Create a DebertaV3Classifier backbone\nbackbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n    CFG.preset,\n)\n\n# (batch_size, 2, sequence_length)\n# Compute embeddings for first response: (P + R_A) using backbone\nresponse_a = {k: v[:, 0, :] for k, v in inputs.items()}\n\nembed_a = backbone(response_a)\n\n# Compute embeddings for second response: (P + R_B), using the same backbone\nresponse_b = {k: v[:, 1, :] for k, v in inputs.items()}\n\nembed_b = backbone(response_b)\n\n# Compute final output \n# axis=-1 表示在 最后一个维度 上进行拼接\nembeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])\nembeds = keras.layers.GlobalAveragePooling1D()(embeds)\noutputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(embeds)\nmodel = keras.Model(inputs, outputs)\n# Compile the model with optimizer, loss, and metrics\nmodel.compile(\n    optimizer=keras.optimizers.Adam(5e-6),\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),\n    metrics=[\n        log_loss,\n        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n    ],\n)","metadata":{"papermill":{"duration":8.231591,"end_time":"2024-06-03T17:02:55.802843","exception":false,"start_time":"2024-06-03T17:02:47.571252","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:47.301571Z","iopub.execute_input":"2025-02-24T05:10:47.301851Z","iopub.status.idle":"2025-02-24T05:10:56.148694Z","shell.execute_reply.started":"2025-02-24T05:10:47.301826Z","shell.execute_reply":"2025-02-24T05:10:56.1477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Summary","metadata":{"papermill":{"duration":0.018012,"end_time":"2024-06-03T17:02:55.83988","exception":false,"start_time":"2024-06-03T17:02:55.821868","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.051632,"end_time":"2024-06-03T17:02:55.909391","exception":false,"start_time":"2024-06-03T17:02:55.857759","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:56.150072Z","iopub.execute_input":"2025-02-24T05:10:56.150787Z","iopub.status.idle":"2025-02-24T05:10:56.177535Z","shell.execute_reply.started":"2025-02-24T05:10:56.150737Z","shell.execute_reply":"2025-02-24T05:10:56.176671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```\ninputs = {\n    \"token_ids\":   <形状为 (batch_size, 2, sequence_length) 的张量>,\n    \"padding_mask\": <形状为 (batch_size, 2, sequence_length) 的张量>\n}\n\ntoken_ids = [\n    [[101, 202, 303, 404, 505],  # 样本1的选项1的token_ids\n    [[606, 707, 808, 909, 100]    # 样本1的选项2的token_ids\n    ],\n    [[111, 222, 333, 444, 555],  # 样本2的选项1的token_ids\n     [[666, 777, 888, 999, 1000]] # 样本2的选项2的token_ids\n    ]\n]\n\npadding_mask = [\n    [[1, 1, 1, 0, 0],  # 样本1的选项1的padding_mask\n     [1, 1, 1, 1, 0]    # 样本1的选项2的padding_mask\n    ],\n    [[1, 1, 0, 0, 0],  # 样本2的选项1的padding_mask\n     [1, 1, 1, 1, 1]    # 样本2的选项2的padding_mask\n    ]\n]\n\n步骤 2: 对每个键值对进行处理\n每次迭代会处理一个键值对：\n\n第一次迭代:\nk = \"token_ids\"\nv = token_ids_tensor（形状为 (2, 2, 5)）\n切片操作 v[:, 0, :]:\n: 表示保留所有批次（即 batch_size=2）。\n0 表示选择第二个维度的第一个元素（即 选项1）。\n: 表示保留所有序列位置的 token。\n结果:\n\n[\n    [101, 202, 303, 404, 505],  # 样本1的选项1\n    [111, 222, 333, 444, 555]   # 样本2的选项1\n]\n将结果存入字典:\n\n\nresponse_a[\"token_ids\"] = 切片后的结果\n第二次迭代:\n\nk = \"padding_mask\"\nv = padding_mask_tensor（形状为 (2, 2, 5)）\n切片操作 v[:, 0, :]:\n: 表示保留所有批次。\n0 表示选择第二个维度的第一个元素（即 选项1）。\n: 表示保留所有序列位置的 mask。\n结果:\n[\n    [1, 1, 1, 0, 0],  # 样本1的选项1的padding_mask\n    [1, 1, 0, 0, 0]   # 样本2的选项1的padding_mask\n]\n将结果存入字典:\nresponse_a[\"padding_mask\"] = 切片后的结果\n\n步骤 3: 生成最终的 response_a\n最终生成的 response_a 字典如下：\nresponse_a = {\n    \"token_ids\": [\n        [101, 202, 303, 404, 505],\n        [111, 222, 333, 444, 555]\n    ],\n    \"padding_mask\": [\n        [1, 1, 1, 0, 0],\n        [1, 1, 0, 0, 0]\n    ]\n}\n\n\nDeBERTa V3 模型会对输入的 token IDs 和 padding mask 进行嵌入（embedding）、位置编码（positional encoding）和多层自注意力机制（self-attention）等操作，最终生成每个 token 的上下文感知的嵌入表示。\n\n\n\n```","metadata":{}},{"cell_type":"markdown","source":"### Model Plot\n\nIn the model graph below, it may seem there are **four** inputs, but actually, there are **two** as discussed before. Our input consists of two parts, one for each response. However, for each input, we have `token_ids` and `padding_mask`, which makes it look like we have four inputs, but in reality, we have two inputs.","metadata":{"papermill":{"duration":0.018914,"end_time":"2024-06-03T17:02:55.947016","exception":false,"start_time":"2024-06-03T17:02:55.928102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Currently throwing error !! [probably library or env issue, so hopefully will be fixed soon]\n\n# keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.025569,"end_time":"2024-06-03T17:02:55.991355","exception":false,"start_time":"2024-06-03T17:02:55.965786","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:10:56.178448Z","iopub.execute_input":"2025-02-24T05:10:56.178704Z","iopub.status.idle":"2025-02-24T05:10:56.182259Z","shell.execute_reply.started":"2025-02-24T05:10:56.17868Z","shell.execute_reply":"2025-02-24T05:10:56.181355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚂 | Training","metadata":{"papermill":{"duration":0.018413,"end_time":"2024-06-03T17:02:56.028558","exception":false,"start_time":"2024-06-03T17:02:56.010145","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Start training the model\nhistory = model.fit(\n    train_ds,\n    epochs=CFG.epochs,\n    validation_data=valid_ds,\n    callbacks=[lr_cb, ckpt_cb]\n)","metadata":{"_kg_hide-input":true,"papermill":{"duration":8128.14365,"end_time":"2024-06-03T19:18:24.191101","exception":false,"start_time":"2024-06-03T17:02:56.047451","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T09:03:54.591206Z","iopub.execute_input":"2025-02-24T09:03:54.591568Z","iopub.status.idle":"2025-02-24T09:03:54.799418Z","shell.execute_reply.started":"2025-02-24T09:03:54.591535Z","shell.execute_reply":"2025-02-24T09:03:54.798276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Best Model\n\nAfter training, let's load the weight with best result to get the best performance.","metadata":{"papermill":{"duration":0.805618,"end_time":"2024-06-03T19:18:25.735457","exception":false,"start_time":"2024-06-03T19:18:24.929839","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/model.weights.h5')","metadata":{"papermill":{"duration":2.546342,"end_time":"2024-06-03T19:18:29.025014","exception":false,"start_time":"2024-06-03T19:18:26.478672","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🧪 | Prediction","metadata":{"papermill":{"duration":0.754041,"end_time":"2024-06-03T19:18:30.53816","exception":false,"start_time":"2024-06-03T19:18:29.784119","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build test dataset\ntest_texts = test_df.options.tolist()\ntest_ds = build_dataset(test_texts,\n                         batch_size=min(len(test_df), CFG.batch_size),\n                         shuffle=False)","metadata":{"papermill":{"duration":1.156834,"end_time":"2024-06-03T19:18:32.520369","exception":false,"start_time":"2024-06-03T19:18:31.363535","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-24T09:00:30.892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions using the trained model on test data\ntest_preds = model.predict(test_ds, verbose=1)","metadata":{"papermill":{"duration":6.510153,"end_time":"2024-06-03T19:18:39.767367","exception":false,"start_time":"2024-06-03T19:18:33.257214","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-24T09:00:30.892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📬 | Submission\n\nFollowing code will prepare the submission file.","metadata":{"papermill":{"duration":0.768879,"end_time":"2024-06-03T19:18:41.329256","exception":false,"start_time":"2024-06-03T19:18:40.560377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub_df = test_df[[\"id\"]].copy()\nsub_df[CFG.class_names] = test_preds.tolist()\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","metadata":{"papermill":{"duration":0.751809,"end_time":"2024-06-03T19:18:42.839827","exception":false,"start_time":"2024-06-03T19:18:42.088018","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-24T09:00:30.893Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔭 | Future Directions\n\nIn this notebook, we've achieved a good score with a small model and modest token length. But there's plenty of room to improve. Here's how:\n\n1. Try bigger models like `Deberta-Base` or `Deberta-Small`, or even LLMs like `Gemma`.\n2. Increase max token length to reduce loss of data.\n3. Use a five-fold cross-validation and ensemble to make the model robust and get better scores.\n4. Add augmentation like shuffling response orders for more robust performance.\n5. Train for more epochs.\n6. Tune the learning rate scheduler.","metadata":{"papermill":{"duration":0.740084,"end_time":"2024-06-03T19:18:44.386408","exception":false,"start_time":"2024-06-03T19:18:43.646324","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 📌 | Reference\n\n* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)\n* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)","metadata":{"papermill":{"duration":0.830152,"end_time":"2024-06-03T19:18:45.950588","exception":false,"start_time":"2024-06-03T19:18:45.120436","status":"completed"},"tags":[]}}]}